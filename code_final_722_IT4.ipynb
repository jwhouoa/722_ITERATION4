{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "######ITERATION 4 CODE######\n",
    "\n",
    "\n",
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-2.1.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('INFOSYS_722_IT4_Jack_Whooley').getOrCreate()\n",
    "\n",
    "###\n",
    "\n",
    "hd1 = spark.read.csv(\"./heartdisease1.csv\",  header=\"true\" )\n",
    "\n",
    "###\n",
    "\n",
    "hd2 = spark.read.csv(\"./heartdisease2.csv\",  header=\"true\" )\n",
    "\n",
    "###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+\n",
      "|summary|              age|               sex|     chestpaintype|       restingbps|       cholesterol| fastingbloodsugar|        restingecg|      maxheartrate|    exerciseangina|           oldpeak|           STslope|             target|\n",
      "+-------+-----------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+\n",
      "|  count|             1190|              1189|              1189|             1189|              1189|              1190|              1190|              1190|              1190|              1190|              1190|               1190|\n",
      "|   mean|53.72016806722689|0.7636669470142977|3.2321278385197645|132.1555929352397|210.30193439865434|0.2134453781512605|0.6983193277310924| 139.7327731092437|0.3873949579831933|0.9227731092436976|1.6243697478991597| 0.5285714285714286|\n",
      "| stddev| 9.35820279763539|0.4250078352526093|0.9356090206575676|18.37644644452336|101.44065209238535|0.4099117568473307|0.8703588379852837|25.517635548982884|0.4873599295179116|1.0863372185219866| 0.610459213954163|0.49939287903118684|\n",
      "|    min|               28|                 0|                 1|                0|                 0|                 0|                 0|               100|                 0|              -0.1|                 0|                  0|\n",
      "|    max|               77|                 1|                 4|               98|                85|                 1|                 2|                99|                 1|               6.2|                 3|                  1|\n",
      "+-------+-----------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#appending hd1 to hd2\n",
    "\n",
    "hd = hd1.union(hd2)\n",
    "\n",
    "hd.describe().show()\n",
    "##\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating two new variables that I hypothesise could have predictive power. I have chained \n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import log\n",
    "\n",
    "hdaddvar = hd.withColumn(\"maxHRrestingHR\",  col(\"maxheartrate\") * col(\"restingbps\")).withColumn(\"MaxHRLogAge\", col(\"maxheartrate\") * log(col(\"age\")))\n",
    "\n",
    "\n",
    "hd.withColumn(\"sqrtcholesterol\",   pow(col(\"cholesterol\",0.5))\n",
    "\n",
    "##\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating discrete age categories out of continuous age variable\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "              \n",
    "hdnewer = hdaddvar.withColumn(\"young\", (when(col(\"age\") < 35, 1).otherwise(0))).withColumn(\"middleaged\", (when(col(\"age\") < 35, 0).when(col(\"age\") > 65, 0).otherwise(1))).withColumn(\"old\", (when(col(\"age\") > 65, 1).otherwise(0)))\n",
    "\n",
    "##\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-----------------+------------------+-------------------+------------------+-------------------+\n",
      "|summary|              age|               sex|     chestpaintype|       restingbps|       cholesterol| fastingbloodsugar|        restingecg|      maxheartrate|     exerciseangina|           oldpeak|           STslope|            target|   maxHRrestingHR|       MaxHRLogAge|              young|        middleaged|                old|\n",
      "+-------+-----------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-----------------+------------------+-------------------+------------------+-------------------+\n",
      "|  count|             1186|              1186|              1186|             1186|              1186|              1186|              1186|              1186|               1186|              1186|              1186|              1186|             1186|              1186|               1186|              1186|               1186|\n",
      "|   mean|53.70826306913997|0.7630691399662732| 3.231028667790894|132.1610455311973|210.29763912310287|0.2133220910623946|0.6981450252951096|139.82967959527824|0.38617200674536256|0.9212478920741989|1.6222596964586846|0.5278246205733558|18432.23187183811| 553.0048455878867|0.02023608768971332|0.8853288364249579|0.09443507588532883|\n",
      "| stddev|9.370798480040056|0.4253788862924378|0.9362363585167415|18.39478554583472|101.56768620234253| 0.409826052388345| 0.870489420054436|25.492773780087944|0.48707619978515787|1.0877470826036968| 0.609885058004891|0.4994357892789875|4049.127246405388| 94.61494864491667| 0.1408663186961068|0.3187590941914979|0.29255641960227186|\n",
      "|    min|               28|                 0|                 1|                0|                 0|                 0|                 0|               100|                  0|              -0.1|                 0|                 0|              0.0|235.90953796345954|                  0|                 0|                  0|\n",
      "|    max|               77|                 1|                 4|               98|                85|                 1|                 2|                99|                  1|               6.2|                 3|                 1|          37440.0| 777.8518890800335|                  1|                 1|                  1|\n",
      "+-------+-----------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-----------------+------------------+-------------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hd3 = hdnewer.na.drop()\n",
    "\n",
    "hd3.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# A few things we need to do before Spark can accept the data!\n",
    "# It needs to be in the form of two columns: \"label\" and \"features\".\n",
    "\n",
    "# Import VectorAssembler and Vectors\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age',\n",
       " 'sex',\n",
       " 'chestpaintype',\n",
       " 'restingbps',\n",
       " 'cholesterol',\n",
       " 'fastingbloodsugar',\n",
       " 'restingecg',\n",
       " 'maxheartrate',\n",
       " 'exerciseangina',\n",
       " 'oldpeak',\n",
       " 'STslope',\n",
       " 'target',\n",
       " 'maxHRrestingHR',\n",
       " 'MaxHRLogAge',\n",
       " 'young',\n",
       " 'middleaged',\n",
       " 'old']"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hd3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all features into one vector named features.\n",
    "assembler = VectorAssembler(\n",
    "  inputCols=['age',\n",
    " 'sex',\n",
    " 'chestpaintype',\n",
    " 'restingbps',\n",
    " 'cholesterol',\n",
    " 'fastingbloodsugar',\n",
    " 'restingecg',\n",
    " 'maxheartrate',\n",
    " 'exerciseangina',\n",
    " 'oldpeak',\n",
    " 'STslope',\n",
    " 'maxHRrestingHR',\n",
    " 'MaxHRLogAge',\n",
    " 'young',\n",
    " 'middleaged',\n",
    " 'old'],\n",
    "              outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- sex: integer (nullable = true)\n",
      " |-- chestpaintype: integer (nullable = true)\n",
      " |-- restingbps: integer (nullable = true)\n",
      " |-- cholesterol: integer (nullable = true)\n",
      " |-- fastingbloodsugar: integer (nullable = true)\n",
      " |-- restingecg: integer (nullable = true)\n",
      " |-- maxheartrate: integer (nullable = true)\n",
      " |-- exerciseangina: integer (nullable = true)\n",
      " |-- oldpeak: integer (nullable = true)\n",
      " |-- STslope: integer (nullable = true)\n",
      " |-- target: integer (nullable = true)\n",
      " |-- maxHRrestingHR: integer (nullable = true)\n",
      " |-- MaxHRLogAge: integer (nullable = true)\n",
      " |-- young: integer (nullable = false)\n",
      " |-- middleaged: integer (nullable = false)\n",
      " |-- old: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "hd3 = hd3.withColumn(\"age\", hd3[\"age\"].cast(IntegerType()))\n",
    "hd3 = hd3.withColumn(\"sex\", hd3[\"sex\"].cast(IntegerType()))\n",
    "hd3 = hd3.withColumn(\"chestpaintype\", hd3[\"chestpaintype\"].cast(IntegerType()))\n",
    "hd3 = hd3.withColumn(\"restingbps\", hd3[\"restingbps\"].cast(IntegerType()))\n",
    "hd3 = hd3.withColumn(\"cholesterol\", hd3[\"cholesterol\"].cast(IntegerType()))\n",
    "hd3 = hd3.withColumn(\"fastingbloodsugar\", hd3[\"fastingbloodsugar\"].cast(IntegerType()))\n",
    "hd3 = hd3.withColumn(\"restingecg\", hd3[\"restingecg\"].cast(IntegerType()))\n",
    "hd3 = hd3.withColumn(\"maxheartrate\", hd3[\"maxheartrate\"].cast(IntegerType()))\n",
    "hd3 = hd3.withColumn(\"exerciseangina\", hd3[\"exerciseangina\"].cast(IntegerType()))\n",
    "hd3 = hd3.withColumn(\"oldpeak\", hd3[\"oldpeak\"].cast(IntegerType()))\n",
    "hd3 = hd3.withColumn(\"STslope\", hd3[\"STslope\"].cast(IntegerType()))\n",
    "hd3 = hd3.withColumn(\"target\", hd3[\"target\"].cast(IntegerType()))\n",
    "hd3 = hd3.withColumn(\"maxHRrestingHR\", hd3[\"maxHRrestingHR\"].cast(IntegerType()))\n",
    "hd3 = hd3.withColumn(\"MaxHRLogAge\", hd3[\"MaxHRLogAge\"].cast(IntegerType()))\n",
    "\n",
    "hd3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "hd3.toPandas().to_csv(r\"C:\\Users\\jackw\\OneDrive\\Documents\\University 2020\\Semester 2\\INFOSYS 722\\Iteration4_BDAS\\hd44.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's transform the data. \n",
    "output = assembler.transform(hd3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- sex: integer (nullable = true)\n",
      " |-- chestpaintype: integer (nullable = true)\n",
      " |-- restingbps: integer (nullable = true)\n",
      " |-- cholesterol: integer (nullable = true)\n",
      " |-- fastingbloodsugar: integer (nullable = true)\n",
      " |-- restingecg: integer (nullable = true)\n",
      " |-- maxheartrate: integer (nullable = true)\n",
      " |-- exerciseangina: integer (nullable = true)\n",
      " |-- oldpeak: integer (nullable = true)\n",
      " |-- STslope: integer (nullable = true)\n",
      " |-- target: integer (nullable = true)\n",
      " |-- maxHRrestingHR: integer (nullable = true)\n",
      " |-- MaxHRLogAge: integer (nullable = true)\n",
      " |-- young: integer (nullable = false)\n",
      " |-- middleaged: integer (nullable = false)\n",
      " |-- old: integer (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Outlier Removal (Python Code)##\n",
    "\n",
    "##removing null values, calculating z-scores\n",
    "\n",
    "#hd.dropna(axis=0, how='any', thresh=None, subset=None, inplace=True)\n",
    "\n",
    "#hd = hd.select_dtypes(exclude=['object'])\n",
    "\n",
    "#from scipy import stats\n",
    "#hd['zage']=np.abs(stats.zscore(hd.age))\n",
    "#hd['zrestingbps']=np.abs(stats.zscore(hd.restingbps))\n",
    "#hd['zcholesterol']=np.abs(stats.zscore(hd.cholesterol))\n",
    "#hd['zmaxheartrate']=np.abs(stats.zscore(hd.maxheartrate))\n",
    "#hd['zoldpeak']=np.abs(stats.zscore(hd.oldpeak))\n",
    "#hd['zSTslope']=np.abs(stats.zscore(hd.STslope))\n",
    "\n",
    "############################################################\n",
    "\n",
    "#removing if any z-score is greater than 3#\n",
    "\n",
    "#hdnew = hd.drop(hd[(hd['zage'] > 3.0) | (hd['zrestingbps'] > 3.0) | (hd['zcholesterol'] > 3.0) | (hd['zmaxheartrate'] > 3.0) | (hd['zoldpeak'] > 3.0) | (hd['zSTslope'] > 3.0)].index)\n",
    "\n",
    "\n",
    "#hdnew2 = hdnew.drop(['zage', 'zrestingbps', 'zcholesterol', 'zmaxheartrate', 'zoldpeak', 'zSTslope', 'Unnamed: 0'], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Dimensionality Reduction (Python Code)##\n",
    "\n",
    "#from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "#sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "\n",
    "#sel.fit_transform(hdnew2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training and testing set.\n",
    "train_data,test_data = output.randomSplit([0.8,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import the relevant classifiers. \n",
    "from pyspark.ml.classification import DecisionTreeClassifier,GBTClassifier,RandomForestClassifier\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use defaults to make the comparison \"fair\". This simplifies the comparison process.\n",
    "\n",
    "dtc = DecisionTreeClassifier(labelCol='target',featuresCol='features')\n",
    "rfc = RandomForestClassifier(labelCol='target',featuresCol='features', maxBins = 50, numTrees = 5, maxDepth = 5)\n",
    "gbt = GBTClassifier(labelCol='target',featuresCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the models (it's three models, so it might take some time).\n",
    "dtc_model = dtc.fit(train_data)\n",
    "rfc_model = rfc.fit(train_data)\n",
    "gbt_model = gbt.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_predictions = dtc_model.transform(test_data)\n",
    "rfc_predictions = rfc_model.transform(test_data)\n",
    "gbt_predictions = gbt_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start off with binary classification.\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Note that the label column isn't named label, it's named PrivateIndex in this case.\n",
    "my_binary_eval = BinaryClassificationEvaluator(labelCol = 'target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTC\n",
      "0.8688974442988204\n",
      "RFC\n",
      "0.927383682830931\n"
     ]
    }
   ],
   "source": [
    "# This is the area under the curve. This indicates that the data is highly seperable.\n",
    "print(\"DTC\")\n",
    "print(my_binary_eval.evaluate(dtc_predictions))\n",
    "\n",
    "# RFC improves accuracy but also model complexity. RFC outperforms DTC in nearly every situation.\n",
    "print(\"RFC\")\n",
    "print(my_binary_eval.evaluate(rfc_predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import the evaluator.\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select (prediction, true label) and compute test error. \n",
    "acc_evaluator = MulticlassClassificationEvaluator(labelCol=\"target\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_acc = acc_evaluator.evaluate(dtc_predictions)\n",
    "rfc_acc = acc_evaluator.evaluate(rfc_predictions)\n",
    "gbt_acc = acc_evaluator.evaluate(gbt_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the results!\n",
      "----------------------------------------\n",
      "A single decision tree has an accuracy of: 84.87%\n",
      "----------------------------------------\n",
      "A random forest ensemble has an accuracy of: 85.15%\n",
      "----------------------------------------\n",
      "An ensemble using GBT has an accuracy of: 88.80%\n"
     ]
    }
   ],
   "source": [
    "# Let's do something a bit more complex in terms of printing, just so it's formatted nicer. \n",
    "print(\"Here are the results!\")\n",
    "print('-'*40)\n",
    "print('A single decision tree has an accuracy of: {0:2.2f}%'.format(dtc_acc*100))\n",
    "print('-'*40)\n",
    "print('A random forest ensemble has an accuracy of: {0:2.2f}%'.format(rfc_acc*100))\n",
    "print('-'*40)\n",
    "print('An ensemble using GBT has an accuracy of: {0:2.2f}%'.format(gbt_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(16, {0: 0.0734, 1: 0.027, 2: 0.2071, 3: 0.0323, 4: 0.0668, 5: 0.0161, 6: 0.0023, 7: 0.148, 8: 0.0364, 9: 0.1298, 10: 0.1805, 11: 0.0382, 12: 0.0339, 14: 0.004, 15: 0.0042})"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_model.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
